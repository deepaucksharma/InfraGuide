# Anti-Patterns + Cardinality Matrix

## Overview

§ Kubernetes monitoring presents unique challenges regarding metric cardinality, especially in dynamic environments where ephemeral workloads are common. This chapter explores common anti-patterns that result in cardinality explosions and provides concrete strategies for cardinality management. We'll examine how different observability systems (New Relic, Prometheus, Datadog) handle high-cardinality workloads and the memory/cost implications of unbounded label combinations.

## Cardinality Explained

§ Cardinality in metrics refers to the number of unique time series generated by a monitoring system. Each unique combination of metric name and label key-value pairs creates a distinct time series that must be stored, indexed, and queried. While labels (or tags) provide essential context for metrics, they come with a cost: each additional label multiplies the number of potential time series, which can lead to exponential growth in resource consumption.

§ For example, a simple metric like `http_requests_total` might start with just a few dimensions (method, status code). But as teams add more context (service, pod, namespace, region, team, version, endpoint, customer_id), cardinality can explode from dozens to millions of time series. This "cardinality explosion" is one of the leading causes of production incidents in monitoring systems.

## Cardinality Multiplier Effect

### TB-4A: Label Multiplier Effects

| Label Scenario | Label Count | Values Per Label | Potential Series | Memory Impact (Prometheus) | Memory Impact (New Relic) |
|----------------|-------------|------------------|------------------|----------------------------|---------------------------|
| Minimal | 3 | 5 | 125 | ~150 KB | ~100 KB |
| Standard | 5 | 10 | 100,000 | ~120 MB | ~80 MB |
| Recommended Max | 7 | 15 | 4,782,969 | ~5.7 GB | ~3.8 GB |
| High-Card Risk | 10 | 20 | 10,240,000,000 | ~12.3 TB | ~8.2 TB |
| Anti-pattern | 12 | 25 | 244,140,625,000,000 | ~293 PB | ~195 PB |

### TB-4B: Memory Footprint per 1M Series

| Observability System | Memory Per 1M Series | Notes |
|----------------------|----------------------|-------|
| Prometheus | 1.2 GB | Higher for recording rules |
| New Relic | 800 MB | NRDB optimized storage |
| Datadog | 850 MB | Varies by retention |
| Thanos | 1.5 GB | Includes redundancy overhead |
| VictoriaMetrics | 750 MB | Optimized for cardinality |

### EQ-4A: Series Calculation Formula

```
Total Series = ∏(label_values_count)
```

Where:
- ∏ is the product operator
- label_values_count is the number of unique values for each label

For example, with labels {app, environment, instance} having {10, 3, 25} unique values respectively:

```
Total Series = 10 × 3 × 25 = 750
```

## Common Anti-Patterns

§ The following anti-patterns frequently lead to cardinality explosions in Kubernetes environments:

### 1. Using High-Cardinality Labels

Avoid using these as labels/tags:
- User IDs
- Request IDs
- Session IDs
- IP addresses (especially client IPs)
- Timestamps or date components
- URL paths without normalization
- Unbounded status messages

### 2. Pod Name Labels on Long-Term Metrics

§ Kubernetes pod names include a unique hash suffix (e.g., `frontend-3a4b5c6d7e`). Using the full pod name as a label creates a new series every time a pod is rescheduled. Instead, use the deployment or statefulset name, which remains stable.

### CF-4A: Kubernetes Label Best Practices

```yaml
# Anti-pattern: Including pod name in metrics
podMonitor:
  selector:
    matchLabels:
      app: myapp
  podMetricsEndpoints:
  - port: metrics
    metricRelabelings:
    - sourceLabels: [__meta_kubernetes_pod_name]
      action: replace
      targetLabel: pod  # Don't do this for long-term metrics!

# Better pattern: Use stable controller name
podMonitor:
  selector:
    matchLabels:
      app: myapp
  podMetricsEndpoints:
  - port: metrics
    metricRelabelings:
    - sourceLabels: [__meta_kubernetes_pod_controller_name]
      action: replace
      targetLabel: controller
```

### 3. Lack of Label Standardization

§ When teams use inconsistent label schemas, the same logical entity may be labeled differently across services, preventing efficient querying and increasing cardinality. Establish and enforce labeling standards across your organization.

### 4. Missing Cardinality Limits

§ Many systems don't set upper bounds on the number of unique label values, allowing unlimited cardinality growth until system failure.

### RB-4A: Cardinality Control Runbook

1. Identify high-cardinality metrics
   ```
   # For Prometheus
   topk(10, count by (__name__) ({__name__=~".+"}))
   
   # For New Relic
   SELECT uniqueCount(metricName) FROM Metric FACET metricName LIMIT 10
   ```

2. Analyze label combinations
   ```
   # For Prometheus
   count({__name__="http_requests_total"}) by (label1, label2)
   
   # For New Relic
   SELECT uniqueCount(timestamp) FROM Metric WHERE metricName = 'http.requests' FACET label1, label2
   ```

3. Apply remediation:
   - Drop high-cardinality labels
   - Aggregate metrics before ingestion
   - Set up cardinality limiters

## Cardinality Management Strategies

### 1. Label Filtering and Relabeling

§ Configure agents and collectors to drop unnecessary high-cardinality labels before they reach storage.

### CF-4B: OTel Collector Configuration for Label Management

```yaml
processors:
  filter:
    metrics:
      include:
        match_type: regexp
        metric_names:
          - "http.server.request.duration"
      exclude:
        match_type: strict
        attributes:
          - key: user_id
            value: .+
          - key: ip_address
            value: .+
```

### 2. Quantiles Over Raw Distributions

§ Instead of storing individual measurements, calculate and store quantiles (p50, p90, p99) to reduce cardinality while preserving statistical significance for latency-type metrics.

### 3. Time-based Aggregation

§ For metrics not requiring high temporal resolution, aggregate before ingestion (e.g., 1-minute averages instead of 10-second samples).

### 4. Hashing High-Cardinality Values

§ When a high-cardinality dimension contains valuable information, consider hashing values into a fixed number of buckets.

### CF-4C: Example of Hash-Based Bucketing in OTTL

```yaml
processors:
  transform:
    metric_statements:
    - context: metric
      statements:
      - set(attributes["user_segment"], Hash(attributes["user_id"]) % 100) where metric.name == "request.latency"
```

## New Relic's Approach to Cardinality Management

§ New Relic's Dimensional Metric (DM) system uses several techniques to manage cardinality:

1. **Dimension quota limits**: Each account has configurable limits on dimensions per metric
2. **Smart sampling**: Automatic sampling for high-frequency, high-cardinality metrics
3. **Composite metrics**: Combining multiple related metrics to reduce overall cardinality
4. **NRQL query optimizations**: Efficient handling of high-cardinality data during query time

## Conclusion

§ Cardinality management is a critical aspect of observability at scale. By understanding these anti-patterns and implementing appropriate guardrails, teams can build sustainable monitoring systems that provide deep visibility without runaway costs or performance degradation. Remember that effective cardinality management requires both technical solutions and organizational practices around naming conventions and metric governance.

---

**Next Steps:**
- Implement the runbook (RB-4A) in your monitoring infrastructure
- Review current label usage across services against the anti-patterns listed
- Consider implementing a label standardization policy
